{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jVNUO34tSPK",
        "outputId": "2e86340e-519a-4c45-ebc6-53e6697093ba"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.63.tar.gz (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.63-py2.py3-none-any.whl size=23918 sha256=ef580834c2e56e30fc429b7d839a4c895466a9026c28846a279ffeff9811a4fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/87/8b/7ec24486e001d3926537f5f7801f57a74d181be25b11157983\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVw0fX1woTKk",
        "outputId": "1df81990-e424-485e-82de-117a9a238ca8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from pandas_datareader import data as wb\n",
        "import yfinance as yf\n",
        "\n",
        "print('Tensorflow   : {}'.format(tf.__version__))\n",
        "print('Numpy   : {}'.format(np.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow   : 2.6.0\n",
            "Numpy   : 1.19.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "aQuZkqS3puAG",
        "outputId": "380a2aab-49c0-44f3-f369-3404905a4227"
      },
      "source": [
        "start_date = datetime(2004, 9, 13)\n",
        "end_date = datetime(2021, 7, 30)\n",
        "tickers = ['GOOGL']\n",
        "\n",
        "brk = yf.Ticker(tickers[0])\n",
        "\n",
        "hist = brk.history(start= start_date,\n",
        "                   end= end_date)\n",
        "hist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2004-09-13</th>\n",
              "      <td>53.368366</td>\n",
              "      <td>54.259258</td>\n",
              "      <td>53.283283</td>\n",
              "      <td>53.803802</td>\n",
              "      <td>7844148</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-09-14</th>\n",
              "      <td>53.778778</td>\n",
              "      <td>56.056057</td>\n",
              "      <td>53.448448</td>\n",
              "      <td>55.800800</td>\n",
              "      <td>10828960</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-09-15</th>\n",
              "      <td>55.335335</td>\n",
              "      <td>57.172173</td>\n",
              "      <td>55.155155</td>\n",
              "      <td>56.056057</td>\n",
              "      <td>10713076</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-09-16</th>\n",
              "      <td>56.226227</td>\n",
              "      <td>57.957958</td>\n",
              "      <td>55.880882</td>\n",
              "      <td>57.042042</td>\n",
              "      <td>9266324</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004-09-17</th>\n",
              "      <td>57.267265</td>\n",
              "      <td>58.803802</td>\n",
              "      <td>56.831833</td>\n",
              "      <td>58.803802</td>\n",
              "      <td>9472518</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-23</th>\n",
              "      <td>2608.610107</td>\n",
              "      <td>2667.979980</td>\n",
              "      <td>2596.010010</td>\n",
              "      <td>2660.300049</td>\n",
              "      <td>2075300</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-26</th>\n",
              "      <td>2666.570068</td>\n",
              "      <td>2684.739990</td>\n",
              "      <td>2646.030029</td>\n",
              "      <td>2680.699951</td>\n",
              "      <td>1528600</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-27</th>\n",
              "      <td>2685.010010</td>\n",
              "      <td>2687.979980</td>\n",
              "      <td>2602.080078</td>\n",
              "      <td>2638.000000</td>\n",
              "      <td>2735500</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-28</th>\n",
              "      <td>2726.239990</td>\n",
              "      <td>2765.939941</td>\n",
              "      <td>2705.550049</td>\n",
              "      <td>2721.879883</td>\n",
              "      <td>4756500</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-29</th>\n",
              "      <td>2723.000000</td>\n",
              "      <td>2733.219971</td>\n",
              "      <td>2710.860107</td>\n",
              "      <td>2715.550049</td>\n",
              "      <td>1585800</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4250 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Open         High  ...  Dividends  Stock Splits\n",
              "Date                                  ...                         \n",
              "2004-09-13    53.368366    54.259258  ...          0           0.0\n",
              "2004-09-14    53.778778    56.056057  ...          0           0.0\n",
              "2004-09-15    55.335335    57.172173  ...          0           0.0\n",
              "2004-09-16    56.226227    57.957958  ...          0           0.0\n",
              "2004-09-17    57.267265    58.803802  ...          0           0.0\n",
              "...                 ...          ...  ...        ...           ...\n",
              "2021-07-23  2608.610107  2667.979980  ...          0           0.0\n",
              "2021-07-26  2666.570068  2684.739990  ...          0           0.0\n",
              "2021-07-27  2685.010010  2687.979980  ...          0           0.0\n",
              "2021-07-28  2726.239990  2765.939941  ...          0           0.0\n",
              "2021-07-29  2723.000000  2733.219971  ...          0           0.0\n",
              "\n",
              "[4250 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGvsBKPuZEBP"
      },
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_3SDP8aws6Q"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['Change_of_price'] = hist['Close'].diff()\n",
        "n = 14\n",
        "\n",
        "def RSI(n, df):\n",
        "  up_df, down_df = df['Change_of_price'].copy(), df['Change_of_price'].copy()\n",
        "  up_df, down_df = pd.DataFrame(up_df), pd.DataFrame(down_df)\n",
        "\n",
        "  up_df.loc['Change_of_price'] = up_df.loc[(up_df['Change_of_price'] < 0, 'Change_of_price')] = 0\n",
        "  down_df.loc['Change_of_price'] = down_df.loc[(down_df['Change_of_price'] > 0, 'Change_of_price')] = 0\n",
        "  down_df['Change_of_price'] = down_df['Change_of_price'].abs()\n",
        "\n",
        "  ewma_up = up_df['Change_of_price'].transform(lambda x: x.ewm(span= n).mean())\n",
        "  ewma_down = down_df['Change_of_price'].transform(lambda x: x.ewm(span= n).mean())\n",
        "\n",
        "  relative_strength = ewma_up / ewma_down\n",
        "  relative_strength_index = 100.0 - (100.0 / (1.0 + relative_strength))\n",
        "\n",
        "  return relative_strength_index\n",
        "\n",
        "def SO(n, df):\n",
        "  low_n, high_n = df['Low'].copy(), df['High'].copy()\n",
        "  low_n, high_n = pd.DataFrame(low_n), pd.DataFrame(high_n)\n",
        "\n",
        "  low_n = low_n['Low'].transform(lambda x: x.rolling(window= n).min())\n",
        "  high_n = high_n['High'].transform(lambda x: x.rolling(window= n).max())\n",
        "\n",
        "  so = 100 * ((df['Close'] - low_n) / (high_n - low_n))\n",
        "\n",
        "  return so\n",
        "\n",
        "def WILR(n, df):\n",
        "  low_n, high_n = df['Low'].copy(), df['High'].copy()\n",
        "  low_n, high_n = pd.DataFrame(low_n), pd.DataFrame(high_n)\n",
        "\n",
        "  low_n = low_n['Low'].transform(lambda x: x.rolling(window= n).min())\n",
        "  high_n = high_n['High'].transform(lambda x: x.rolling(window= n).max())\n",
        "\n",
        "  wr = (-100) * ((high_n - df['Close']) / (high_n - low_n))\n",
        "\n",
        "  return wr\n",
        "\n",
        "def MACD(df):\n",
        "  ema_26 = df['Close'].transform(lambda x: x.ewm(span= 26).mean())\n",
        "  ema_12 = df['Close'].transform(lambda x: x.ewm(span= 12).mean())\n",
        "\n",
        "  macd = ema_12 - ema_26\n",
        "  ema = macd.ewm(span= 9).mean()\n",
        "\n",
        "  return macd, ema\n",
        "\n",
        "def PRC(n, df):\n",
        "  prc = df['Close'].transform(lambda x: x.pct_change(periods= n))\n",
        "  return prc\n",
        "\n",
        "def OBV(df):\n",
        "  volume = df['Volume']\n",
        "  change = df['Close'].diff()\n",
        "\n",
        "  pre_obv = 0\n",
        "  obv_l = []\n",
        "\n",
        "  for i,j in zip(change, volume):\n",
        "    if i > 0:\n",
        "      current_obv = pre_obv + j\n",
        "    elif i < 0:\n",
        "      current_obv = pre_obv - j\n",
        "    else:\n",
        "      current_obv = pre_obv\n",
        "    pre_obv = current_obv\n",
        "    obv_l.append(pre_obv)\n",
        "  \n",
        "  return pd.Series(obv_l, index= df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_P0T8yI_RfD",
        "outputId": "d273394a-5c5a-48e5-939c-76186966467d"
      },
      "source": [
        "df['RSI'] = RSI(n, df)\n",
        "df['SO'] = SO(n, hist)\n",
        "df['WILR'] = WILR(n, hist)\n",
        "df['MACD'], df['MACD_EMA'] = MACD(hist)\n",
        "df['PRC'] = PRC(n, hist)\n",
        "df['OBV'] = OBV(hist)\n",
        "df['CLOSE'] = hist['Close'].values\n",
        "df = df.dropna()\n",
        "print(df.shape)\n",
        "\n",
        "cols = list(df)[:]\n",
        "print(cols)\n",
        "\n",
        "df = df[cols].astype(float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4236, 9)\n",
            "['Change_of_price', 'RSI', 'SO', 'WILR', 'MACD', 'MACD_EMA', 'PRC', 'OBV', 'CLOSE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4byvleGt140",
        "outputId": "1efa0275-7312-44f0-9df0-1c0bc065d167"
      },
      "source": [
        "X_train = df[df.index <= '2020-07-30'].copy()\n",
        "X_test = df[df.index > '2020-07-30'].copy()\n",
        "y_train = X_train['CLOSE'].copy()\n",
        "y_test = X_test['CLOSE'].copy()\n",
        "\n",
        "X_train = X_train.drop(['CLOSE'], axis=1)\n",
        "X_test = X_test.drop(['CLOSE'], axis=1)\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3985, 8) (251, 8)\n",
            "(3985,) (251,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk8rqTF-_24w"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "tmp_sc = StandardScaler()\n",
        "\n",
        "df = tmp_sc.fit_transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJdN-MJuQMg_"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X_scaler = StandardScaler()\n",
        "Y_scaler = StandardScaler()\n",
        "\n",
        "X_train, X_test = np.array(X_train), np.array(X_test)\n",
        "y_train, y_test = np.array(y_train), np.array(y_test)\n",
        "y_train, y_test = np.reshape(y_train, (-1,1)), np.reshape(y_test, (-1,1))\n",
        "\n",
        "X_train = X_scaler.fit_transform(X_train)\n",
        "X_test = X_scaler.transform(X_test)\n",
        "\n",
        "y_train = Y_scaler.fit_transform(y_train)\n",
        "y_test = Y_scaler.transform(y_test)\n",
        "y_train, y_test = np.reshape(y_train, (-1)), np.reshape(y_test, (-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awFO0nO9RCWZ"
      },
      "source": [
        "def create_batches(df, n_future, n_past, dfy = None):\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  if isinstance(df, (np.ndarray)) and dfy is not None:\n",
        "    for i in range(n_past, df.shape[0]):\n",
        "      X.append(df[i - n_past:i])\n",
        "      Y.append(dfy[i])\n",
        "  else:\n",
        "    for i in range(n_past, len(df) - n_future +1):\n",
        "        X.append(df[i - n_past:i, 0:df.shape[1] - 1])\n",
        "        Y.append(df[i + n_future - 1:i + n_future, -1])\n",
        "  \n",
        "  X, Y = np.array(X), np.array(Y)\n",
        "  if Y.ndim == 1:\n",
        "    Y = np.reshape(Y, (-1,1))\n",
        "  print(f'X shape:  {X.shape}')\n",
        "  print(f'Y shape:  {Y.shape}')\n",
        "  return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G62zDVGH_dTK",
        "outputId": "687ed93d-921e-4a83-eea2-6e5c22abdcb4"
      },
      "source": [
        "n_f = 1\n",
        "n_p = 60\n",
        "\n",
        "X, y = create_batches(df, n_f, n_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape:  (4176, 60, 8)\n",
            "Y shape:  (4176, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBMQw2Sm8E1p",
        "outputId": "e8185aae-25c5-408d-fd9e-79b4b9c1467c"
      },
      "source": [
        "X_train, y_train = create_batches(X_train, n_f, n_p, y_train)\n",
        "X_test, y_test = create_batches(X_test, n_f, n_p, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape:  (3925, 60, 8)\n",
            "Y shape:  (3925, 1)\n",
            "X shape:  (191, 60, 8)\n",
            "Y shape:  (191, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNIOo-16yyI7"
      },
      "source": [
        "##LRfinder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzbhn0XyyIIK"
      },
      "source": [
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import keras.backend as K\n",
        "\n",
        "class LRFinder:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if math.isnan(loss) or loss > self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        num_batches = epochs * x_train.shape[0] / batch_size\n",
        "        self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
        "\n",
        "        # Save weights into a file\n",
        "        initial_weights = self.model.get_weights()\n",
        "        # self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.set_weights(initial_weights)\n",
        "        # self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale('log')\n",
        "\n",
        "    def get_derivatives(self, sma=1):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma=1, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        best_der_idx = np.argmin(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5JEBQa9ZIN6"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXs3k5sOWeC6"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.8, \n",
        "                              patience=5, \n",
        "                              min_lr=0.001, \n",
        "                              verbose=1)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', \n",
        "                   mode='auto', \n",
        "                   verbose=1, \n",
        "                   patience=11)\n",
        "\n",
        "lr = 0.005\n",
        "opt = Adam(learning_rate= lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWIKY8WRUv2g",
        "outputId": "a1b02a24-412a-4a42-cc27-1386a4e8afd7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(30, activation='relu', return_sequences= True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(40, activation='relu', return_sequences= False)))\n",
        "model.add(Dropout(0.3))\n",
        "# model.add(Bidirectional(LSTM(70, activation='relu', return_sequences= False)))\n",
        "# model.add(Dropout(0.4))\n",
        "model.add(Dense(y_train.shape[1]))\n",
        "\n",
        "model.compile(optimizer= opt, loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_4 (Bidirection (None, 60, 60)            9360      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 60, 60)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 80)                32320     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 81        \n",
            "=================================================================\n",
            "Total params: 41,761\n",
            "Trainable params: 41,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "BRdrBT5j0wk3",
        "outputId": "c305fe42-c980-4813-b259-4d03c6cdf826"
      },
      "source": [
        "bs = 32\n",
        "\n",
        "lr_finder = LRFinder(model)\n",
        "lr_finder.find(X_train, y_train, \n",
        "               start_lr=0.0000001, end_lr=100, batch_size= bs, epochs=5)\n",
        "lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "123/123 [==============================] - 86s 656ms/step - loss: 0.9266\n",
            "Epoch 2/5\n",
            "123/123 [==============================] - 78s 635ms/step - loss: 0.8511\n",
            "Epoch 3/5\n",
            "123/123 [==============================] - 40s 326ms/step - loss: 9.0769\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUZb7H8c9vShqpQCCkkNA7BAlFwbauihW72FZWV3bvrm53F+/e67qW1at3dW3rrq7l6tpFXawoNgSkhA6BQGghBEjvffLcP2YSQhgggTmZSeb3fr14vTJnzpz55bzCfOd5nnOeR4wxKKWUCl42fxeglFLKvzQIlFIqyGkQKKVUkNMgUEqpIKdBoJRSQU6DQCmlgpzD3wV0Vt++fU1aWpq/y1BKqW5l9erVRcaYeG/PdbsgSEtLIzMz099lKKVUtyIie472nHYNKaVUkNMgUEqpIKdBoJRSQU6DQCmlgpwGgVJKBTkNAqWUCnIaBF7sL6+loKLO32UopVSX0CDwYu7Lq7nuueU0upr9XYpSSllOg6Cd8tpGNuWXs6OwmtdX5vq7HKWUsly3u7PYamtySzEG+kWFcs+CzXy5tYCh8ZH818Wj/V2aUkpZImhaBHWNLr7dXnjc/TJ3l+CwCR/+fAbXTh7I1v2V/HPJLrYdrOyCKpVSqusFTRA89WUOc15cxc7CKq/PG2P49VvrePm7PYxJiqFfVBgPXjGOj34+A4dNeDtzbxdXfKTm5pNfX7q8tpFHFm5lV1G1DypSSvUEQRMEc6anEeqw8ZfPt3l9fmlOMe+u2ceQ+Ejmnj64dXufyFC+N7If763N98kH8Yk4WFHHbS9nkn7vZ+w4SpB11EOfbOXpr3Zw/l8Xc/b/fs07q/Mor23EGP/8bkop/wuaIOgbGcot0wfx0Yb97Cl2fxuua3Qx9+VMsvIreOabHPpFhfLmj6dx0fgBh7125tgEiqrq2XrAuu6hJduLOP+xxTzaLqhqG1zc8tIqlmwvotnAXe9uPKFAyimo4qbnV/DGqlyumpTMDVMHEh3u5Ldvr2fCnz7j0qeWsiynSANBqSAUVIPF12Sk8NRXOXyedZCrM1LIyq/gs6yDlNU0snJ3Cb+bOYJQh/2I100d3AeAFbuKGZ0Y7fO6XM2Gn/xrNTUNTeQuruGW6WnERoQA8NAnW8jaX8HzN2dQUFHPvHc38tHG/VwyIbHDx29uNvznexvZsr+C66cM5K4LRxEZ6qDR1cxrK3Ipq2nktZV7uP6fKwh32jlrRDwPXTmemHDnEccyxrA0p5iVu0sYlRBFr1AHk1Lj6BV65J9S9oFKsg9WYhN311xEiJ0HLh/HdzuKmb8mj5S4CKYN7s2g+Ej2FFczND6SpLhwwp12+kWHnfgJVUp1SlAFwcA+EYxMiOJvX+/ggY+3MHNMAgArd5dgE7jqlGSvr0uKDSc5LpwVO0v44fRBPq9rV1E1VfVN3Hb6IJ77dhf3LNhMv+gwKusaeXPVXm6cmsr3RvbH1Wx4adluHlmYTW2Di0vTEwlzHh5cNQ1N/GlBFsXV9Vw7eSAL1uezcNMBGlzN3HfZWG6altq6r9Nu4+bT0gD48ZmDWbAun/V5Zby5ai8HK1Yy/z9OQ0QOO/4bq/Zy17sbD9s2MiGK/7tlCv09H97V9U08/sV2nl+yC5en9TIyIYo9xTVc8Pi3AIxPjmFTfjmfbj5wxPkIc9r4/cyRzJ48kPCQI4NZKeVbQRUEAOeNSeCJL7YD8MmmA9gEmg2cOTz+mN9Cpw7qw1fZBTS5mnHYvfeoGWP40wdZpPWJYI6XwNh6oII/f7yVJ2ant37jb9kOcNnEJCpqm3gzcy9Ou2ATISbcya/PHQ6A3SbMu2Akc15cxe/mb2D5rmIevSYdgKKqet5YmcurK3I5UFFHTLiTRVsKEIHLJybRK8TBdZNTjvr7hTntXDM5hWsmpzA2KYa73t3IF1sK+P7o/q377Cqq5v4PszhtSB+e/UEG2Qcq2FtSyx/e28gVf1vGK7dOISEmjOueW86GvHJmT05hclpvtuyv4DfnjWBPSTWPfb6N66emcuZw90JJ+WW17C2poX90GIu3F1Lf2Mw32wr50wdZ/HXRduaeMZifnT30qHUrpU6edLc+4YyMDHMyK5Tll9Xy2Ofb2F5Qxbq9ZXx/VH9CnTbmnJbG5LTeR33d51kHue3lTG6ZPoi7LxmNMYavtxUyfUhfQhzuYJi/Oo/fvL2eEIeNM4bFs72gkuumDOTHZwxGRPjVm+t4b+0+fn7OMEYPiOYvn2Vz14UjWbOnjGe+2UHWvecT6rBTXtvYGlD1TS76RR0eUKXVDTy/ZBdPfZXDX66ewOnD+nLhE99SVNXA9KF9uP3sYUwcGMuGvHKiwhyMGtC57qxGVzPff/QbquubmDgwjgPldYQ4bBwor6OmoYkP7phBclxE6/4b88qZ8+JKDJAcF86mfeU8c+Mkzve0uDrLGMOq3aU88PEWNu8rJ+fPF57QcZRSh4jIamNMhtfngi0IWry0dBf3fJDF72aO4Kdndewb5z0LNvPSst38/cZTqKxr4s53NnDPJaOZM30QtQ0uZvzPl8RHhZJTUEVTs2FMYjSb8yu4cdpAhveP4oGPttDk6SpxNRscNqFXqIMBMWE0G8Nnvzqzw/U3uZq5/p8r2LSvnCHxkWw7WMlbPz6VCSmxJ3Q+2lu/t4zHFm0jt6SGlLgIymsb2VlYxYs/nMKk1Lgj9t9dVM3NL66kyWWYd8HITo1hHM1fF23jr4u2s/PPF2KzyfFfoJQ6qmMFQdB1DbW4YNwAXluZy/dG9uvwa/5w0ShW7ynl9/M30vK5tGB9PnOmD+Lt1Xsprm7gmRsnsfVABQ1NzdwyfRC/emsd/1p+aKqKR6+ZwLOLd3L5xCTOGdWPy/+2jK0HKjmnE3UAOOw2npg9kTkvrqSgso4HrxjnsxAAmJASy0s/nNLh/dP69mLRr8/ELuKzD+2WllaDq5kwm44VKGWVoG0RnKidhVXc9e5G9pfXMSEllg/W5zMhOYbsg5WMHhDtdYC1rKaB4uoGDpbXcdrQvoc9tza3lCufWcZ9l43lhqmpqEP++e1O7v9oC+v/eJ7XK5iUUh2nLQIfGhwfyZs/PhWAvSU1LNx0gMr6JmZNSOKmU1OPCAGA2IgQYiNCGBIfecRzEwfGkX3/BTiPMgAdzFpaBDoLrFLW0iA4CSm9I1gy72x6R4Qc9UqijtAQ8C7Ec14amjQIlLKSBsFJan9Fj/Kd1jECDQKlLKVfRVXAajtYrJSyjqVBICIzRSRbRHJEZJ6X51NF5AsR2SAiX4uI91t7VVByateQUl3CsiAQETvwNHABMBq4TkTar+7yv8DLxpjxwL3Ag1bVo7ofbREo1TWsbBFMAXKMMTuNMQ3AG8CsdvuMBr70/PyVl+dVEAvVFoFSXcLKIEgC2q7mkufZ1tZ64ArPz5cDUSLSp/2BRGSuiGSKSGZh4fFXGVM9gw4WK9U1/D1Y/FvgTBFZC5wJ7ANc7XcyxjxrjMkwxmTEx8d3dY3KTzQIlOoaVl4+ug9oO91lsmdbK2NMPp4WgYhEAlcaY8osrEl1I62DxTpGoJSlrGwRrAKGicggEQkBZgML2u4gIn1FpKWGu4AXLKxHdTN6Z7FSXcOyIDDGNAG3AwuBLcBbxpjNInKviFzq2e0sIFtEtgH9gQesqkd1Py13Ftdr15BSlrL0zmJjzMfAx+223d3m53eAd6ysQXVfoTpGoFSX8PdgsVJHpYPFSnUNnWtIBSwdLD6+irpGwhx2GlzNPPrZNuw2uPP8kdQ1uYgO06m7VcdoEKiA1TpYrC0CAMprG9m6v4LfvL2e6DAnzcaQfbCSyFAHrmZDTYP7yut3VudRUdfEtZNT+OMlowl1+GdRn1W7S/jlG+v47fnDuXyizh4TyDQIVMBy2AQRbRFs2lfOT19dQ25JDQBpfSLoExmC027jvDEJHPSsKX3FKUm8tiKXNbmlzBybwGsrcskpqGJWeiIXjRtAbERIl9a9clcJ+8pq+dWb66mqa+KmU9O69P1Vx2kQqIAlIoTYbUE1RlDf5KKhqRmn3cZfF23n6+wCGpqaqWt08fuZI2k2hmsnp9A3MtTr6ycOjDvs54c+2cof3tvE/R9u4b8uHsWE5FhGDYimvsnFb95aT0SIgxEJkUwf2pcxiTE+/V0KK+sJc9qYPqQvdy/YzKebD/DHS8YwvH+UT99HnTwNAhXQQhy2oLp89K75G/lww36iwx0UVTXQNzKUoqp6nr1pEueNSejUsa7JSOHqScls2V/JfR9m8Yf3NgEwvH8k4U47G/eV0yvUwfw1TYQ4bPzy+8MYGh9JUly4T0KhsLKexJhwnrx+In//egevrczlqmeW8fh1E8lIjaO0upGEmLDWLkDlPxoEKqCF2G1B0zVU2+Dik00HSO4dztD4SG6ZMYgJybHkFFQxLvnEPphFhNGJ0fzrR1NZvL2Qwop6Xlm+h5KaBh66YjxXnJJEcXUDv3xjHQ9/mg2ATWDeBSM5dXBfPs86wFkj+zExJdbrMqzHUlhZT9+oUCJCHPz6vBFcnZHCbS9n8sMXV7Xuk9YngntnjeWM4Tp1jD9pEKiAFuKwBcVgcXOz4evsAmobXdw3ayzTh/Ztfe5EQ6Atu004e0Q/AK6ZnHLYc/2jw3h97jT2ldVSVFnP37/ZwZ8/3tr6/BNf5jCsXySzpwzkB6emdnhp1YLKOsYmHao9pXcE7/9sOm+u2kt9k4twp50Xlu7mBy+s5PRhffnJmUMO+71V19EgUAEtxNE9WgTGGDbklRMb4SS1T69OvbbJ1cw1//iONbllxEY4mTKot0VVHltSbDhJseH87YZTeG/tPlbtLuGnZw1laU4Rb2bu5b4PsyitbuC354/o0PEKK+uPWMo1zGnn5tPSWh9fnZHC80t28a/le7jlpVUsuH0GIxJ0DKGraeecCmiBPFicV1pDo8s9kHv9cyuY9fRSbnp+Ja5m06njvLB0F2tyy7gsPZF7Z43t8Dduq4gIV5ySzINXjCeldwSzpwzkvZ9O54pTkvj7Nzv4OrvguMeorm+iusFFfJT3Qe0WYU47Pzt7KAtun0FUmJOLnviWWU8vZdO+cl/9OqoDNAhUQAtxBGYQVNU3ce6ji3n4U/dVOd/tLOay9ERyS2qO+UFZWt1AaXUDVz6zjA835LO7qJq/fLaN80b357Fr07l0QmIX/hadc/fFoxkc34s5L67ikieXsHxn8VH3LaysB6DfcYKgRXxUKK/+aCq3nTGY/LJaLn5yCRc/+S1b9lf4pHZ1bNo1pAKaM0AHi1ftLqG20cVLy3bT6DLc8b2h/PycYSzfWcJLy3Zzzqj+R7wmK7+CS55aQkJ0GPvKask+UElyXDghDhv3XTa204OxXS02IoQFt8/g+SW7eGNVLj97dQ2f/OJ0+kWHHbFvgScIjtciaGtEQhS/nzmSuacPZv6aPJ5dvJNZTy/lJ2cO4fazh+rVRRbSIFABLVBbBMt3FGO3CY0uw+C+vbj9e0Nx2m3cdGoqjyzM5sWluzhYUc/vZ45o/YD/cutBXM2GfWW1XDExiS+zCyitaeCRqybQ38uHaSBq6co5f0x/LnlyKZc9vZTThvalttFFmMNOYmwYvUIdvL/WvfRIv+iOB0GLuF4h/Oj0wcxKT+L+j7J44ovtfLOtkLsvHs2k1LjjH0B1mgaBCmihDhtV9U3+LuMIy3cWM2lgHDdMG8iYxOjWaRxmT07h8S+286cPsgAYnxzDheMGAPDt9iJGD4jm4avGM2pANA1NzYQ6bNhsgd0S8GZovyhenzuNefM38M22QqLCHNQ1uDhYWX/YGEn8UW5864j4qFAenz2RC8YmcNe7G7nymWW8fts0Th1yxGq26iRpEKiAFgiDxWtyS3n4063848YMIsMcPPzpVjbuK+f2s4cyK/3wZbj7RIZy/ZSBLNx8gIgQO/cs2Ez2gUq+P6o/a3JLuWX6oNZLKsND/DMHkK+kp8Ty6S/POGxbZV0jtQ0ucgqq2JRfTu9eJz+txcyxAxgSH8m5jy2moLLupI+njqRBoAKaP7qG/r1uH1v2VzLvgpE0upqZN38D2w5W8cGGfBKiw/jH4p1cMDaBW2YM8vr6uy8ezX9eOIqs/RX88d+bePLL7Tz+xXYAzuzhN05FhTmJCnPSLzqM03x4T0BLaAbTXeZdSYNABTSn3dZlS1U2upppNoaHP81mX1kt54/pT+buUrYdrCIq1MGCdfn0iw4lLsLJE9dNPOplnjabEGIT0lNi+fftM8gvq2VpThGJseHarXGCdG0Ka2kQqIDWFS2CkuoGPtq4n6e+3E6zcV/6KAJ/+iCL7QcrOWdkPyYOjOV/P9tGiMPGNRnJnbrWPzE2nKszUo6/ozqqULu7RaBBYA0NAhXQrL6zuMnVzKynl7C3pJYxidHsKKwiLsLJL78/nPs+zMJpt/HHS8YQFeZgfV45a3PLmD15oGX1KO9aWwQBeClxT6BBoAJaiN3a2Ue/zi5kb0ktf7l6AleckkT2wUoamwzjkmO4OiOZ6vpDd8c+94MMy+pQx6ZdQ9bSIFABLdTirqG3V++lb2QIl6YnIiKMTIhufS4ixEFEiP4XCQR2m2C3iQaBRfRWPRXQrBwsLq9t5MutBVyWnuT3+X3U8QXTlORdTf/6VUALcdhoNu6+fF/7cutBGl2GC8cP8Pmxle+FOGzUN7r8XUaPpO1eFdDaDhI6fPStvaahiR++uIr95XX0jw4lPTnWJ8dV1uouU5J3RxoEKqA5PNMvNLo6N7XzsazZU8aKXSUA3DQttVtO8RCMrL5wIJhpEKiA1tJ378uuoXV7SwG4b9YYLhofuNM+q8NZfeFAMNMgUAGtJQh82SJYm1vGkPhe3HRqms+OqawXqDPR9gQ6WKwCmsPe0jV04h8AK3YWc/rDX1Ja3YAxhnV7y5g4UKcz7m50jMA62iJQAc3pCYKmTi7/2Nbi7e6bxpbtKCYxNozi6gbSU3SAuLsJhJloeyoNAhXQHLaTHyPIPlAJwJdbC8jcU0JCdBgX6yWj3Y52DVlHg0AFtJYxgpPpEtjqCYL5a/IAeOvHpxIbcfLz5KuuFRKgixT1BJaOEYjITBHJFpEcEZnn5fmBIvKViKwVkQ0icqGV9ajup7Vr6AQHi6vqm8grrSU2wgnAZemJTBnU22f1qa6jXUPWsSwIRMQOPA1cAIwGrhOR0e12+y/gLWPMRGA28Der6lHdU8tNZE3NJ/YB0NIt9LOzhjJ1UG/unDnSZ7WprqVdQ9axsmtoCpBjjNkJICJvALOArDb7GKBllq8YIN/CelQ35DzJG8q2HqgAYObYBG47Y7DP6lJdL8ShN5RZxcquoSRgb5vHeZ5tbd0D3CgiecDHwB3eDiQic0UkU0QyCwsLrahVBajWFsEJBsGyHcX0iwolOS7cl2UpPwjVy0ct4+/7CK4DXjLGJAMXAq+IyBE1GWOeNcZkGGMy4uN79pqv6nDOk7iPwNVsWLK9iDOHxyOi00h0dzpGYB0rg2Af0HZ9vmTPtrZuBd4CMMZ8B4QBvlvxWnV7h+4s7vwHwPq8MsprGzlzhH556Al0jMA6VgbBKmCYiAwSkRDcg8EL2u2TC5wDICKjcAeB9v2oVo4O3lBW2+CivunwKYrfX7sPm8CMofrdoifQO4utY1kQGGOagNuBhcAW3FcHbRaRe0XkUs9uvwFuE5H1wOvAHGOM7yaVUd1eyw1lR2sRbDtYSVV9E9c9t5y739/cun3TvnL+tXwP108dqPcM9BAhdjuuZoPrJO4yV95ZekOZMeZj3IPAbbfd3ebnLGC6lTWo7i3kGJPOlVY3cPGTS5h7+mCy8ivILanhwWaDzSa8umIPvUIc3Hm+Xi7aU7Rdtzg8xO7nanoWfw8WK3VMrV1DXloEn2UdoKGpmW+3F9LgaqakuoEtnstFdxVVM6x/JDHhzi6tV1lHF7C3jgaBCmits4966Q74cMN+ADbsK2/dtjSnCIDc4hpS+/TqggpVV2kJgnqXLlfpaxoEKqA5jzLpXFV9E8t2FBNit9EyqhQV5mDZjmLqm1zsr6hjYO+Iri5XWSjU001Y36gtAl/TIFABzXGUuYayD1Tgajatl4baBL43sh+b9lWwt6QWYyC1jwZBT9J2/WrlWxoEKqAdbfbRLfvdcwhdOC4BgAEx4UxIjqWoqp7Ve9zrEWsQ9Cw6RmAdnYZaBTRnuykmSqsb+NHLmdhtQnSYg+lD3PcIDOwdwehE97RVH2884NmmYwQ9ScsVZBoEvqdBoAKa3SaIHJp9dNXuElbvcS8+P2VQb+KjQokJdzIovldrEHyzrZCIEDt9I/X+gZ5Eu4aso0GgAp7TZmu9j2BfWW3r9lEJUYgIr/5oKv2jw4gOO3Sp6PShfXV+oR5Gu4aso0GgAp7TLq1XDe0prgFABMYnu9cdHpsU07rvz88ZRlZ+BY9dm971hSpLaRBYR4NABTyH3dY6xcTu4mrGJEbzxHUTSfNyn8Cvzx3e1eWpLtIyRqBrEvieXjWkAp7TLq03lO0uqiatTy+GxEdit2nXTzAJ1TECy2gQqIDnsNlocjXT6Gomr7SWtL56WWgwCnW45xfSriHf0yBQAc9hF5pchvyyWpqajU4dEaRCne6Pq7pGnWLC1zQIVMALsbvnoW8ZKE7VqSOCUlSYe0izqr7Jz5X0PBoEKuC1bREAJMbq+sPBKNxpx2ETKmob/V1Kj6NBoAKew2ajqbmZ/PI6RCAhJszfJSk/EBGiw51U1GkQ+JoGgQp4TrvQ6DIcKK8lPjK0ddoJFXyiwxxU1Lq7hvYUV1Neo6HgC/o/SgU8p93dIthfXscA7RYKatHhTso9XUOzn13OY4u2+bminkGDQAU8h11obHKPESRqt1BQi/F0DVXUNbK/vI6Cyjp/l9QjaBCogOe022hsaRHEaIsgmEWHOamobSTXcwVZSzeROjkaBCrgOWxCSXUDNQ0uEmO1RRDMosMdVNQ1tV5KXKkDxz6hQaACnsNua/2Pry2C4NbSIthTUg1ARZ22CHxBg0AFvJA2VwkN0BZBUIsOd1Lf1Mz2g1UAek+Bj2gQqIDXsm4xQL+oUD9WovwtOty95sSGvDIAKuoaMcYc6yWqAzQIVMBz2A79mfbppUEQzKI900zsKHR3DTW6jE5L7QMaBCrgOT0tgnCnnfAQu5+rUf7U0iIASPLcU6LdQyevQ0EgIr8QkWhxe15E1ojIeVYXpxQcWsC+dy9dgzjYtV2O9NzR/QF0ygkf6GiL4BZjTAVwHhAH3AQ8ZFlVSrXRMkbQRxejD3ox4YevSw165ZAvdDQIWkbrLgReMcZsbrNNKUtpi0C1iA4/tLpuy9+Ddg2dvI4GwWoR+Qx3ECwUkShAR2hUl3B4lqTUIFCx4e6/gTmnpRHjCQVtEZy8ji5efyuQDuw0xtSISG/gh9aVpdQhjpYWQYQGQbALcdjYeM959ApxUFhVD+jdxb7Q0RbBqUC2MaZMRG4E/gsoP96LRGSmiGSLSI6IzPPy/GMiss7zb5uIlHWufBUMmjyLlbftH1bBKyrMic0mrQPHOt/QyetoEDwD1IjIBOA3wA7g5WO9QETswNPABcBo4DoRGd12H2PMr4wx6caYdOBJ4N1O1q+CQKWn6R+tQaDaCHPacNpFrxrygY4GQZNx3743C3jKGPM0EHWc10wBcowxO40xDcAbntcfzXXA6x2sRwWRlv/obQcKlRIRosKc2jXkAx0NgkoRuQv3ZaMfiYgNON7XsyRgb5vHeZ5tRxCRVGAQ8GUH61FBpOWqkLbXkCsF7juNy7Vr6KR1NAiuBepx309wAEgGHvFhHbOBd4wxLm9PishcEckUkczCwkIfvq3qDiI9ARCv8wypduJ6hVBa3eDvMrq9DgWB58P/VSBGRC4G6owxxxwjAPYBKW0eJ3u2eTObY3QLGWOeNcZkGGMy4uPjO1Ky6kHunzWWh68cz7ikGH+XogJMfGQohZX1/i6j2+voFBPXACuBq4FrgBUictVxXrYKGCYig0QkBPeH/QIvxx6J+27l7zpTuAoeMRFOrpmcgojew6gOFx8V2noZqTpxHR19+wMw2RhTACAi8cAi4J2jvcAY0yQitwMLATvwgjFms4jcC2QaY1pCYTbwhtG5ZJVSndQ3MpTSmgYaXc2td6CrzutoENhaQsCjmA60JowxHwMft9t2d7vH93SwBqWUOkx8VCjGQEl1A/2jddGiE9XRIPhURBZyqB//Wtp9wCulVFdruYCgsLJeg+AkdCgIjDF3isiVwHTPpmeNMe9ZV5ZSSh1f30hPEOg4wUnp8B06xpj5wHwLa1FKqU5pWbp09e5SBsSEMTIh2s8VdU/H7OcXkUoRqfDyr1JEKrqqSKWU8qalRfDUVznM/Ou3fq6m+zpmi8AYc7xpJJRSym906VLf0OutlFI9hl6FfmI0CJRS3Vpan4jWn6sbvM5So45Dg0Ap1a198oszuG/WGACdd+gEaRAopbq18BA7A2LCAfeNZarzNAiUUt1enGc965IaDYIToUGglOr2enuCQLuGTowGgVKq2+sd4QmCGvciRjkFldzx+loe/GQLjZ41r9XR6dp/SqluLyrMgd0mPL5oGx9uyGdyWm8+WJ8PwOgB0cxK97o4ovLQFoFSqtuz2YS4CCcVdU2szS1jwbp8JqTEktYngle+2+Pv8gKeBoFSqkcIcx66y/hARR2TBsZx47RUMveUsn5vmR8rC3waBEqpHiGvtPawx6ekxnLt5BR69wrhfz7dqncdH4MGgVKqR5k5JgGAiQPjiApzcsf3hrJsRzHfbCv0c2WBS4NAKdUjPHLVeGZPTuFX5w7ndzNHkBjjXqjmhqmpDOwdwUOfbMXVrK0CbzQIlFI9wtUZKTx05XhGJETx07OGIiIAhDhs3Hn+CLYeqOTzrAN+rjIwaRAopXq8C8cNIDbCyWebD/q7lICkQaCU6vHsNuHsEf34KrtAu4e80CBQSgWF743sR2lNI+v2lvq7lICjQaCUCgpnDI8nMtTBQ59spUmnnTiMBoFSKijEhDt54PKxrNpdyuur9vq7nICiQaCUChqz0pNI6xPBku16TwT5yd0AABEkSURBVEFbGgRKqaBySmocq/eU6p3GbWgQKKWCyqTUOIqqGsgtqfF3KQFDg0ApFVQmpcYBkLlbrx5qoUGglAoqw/pF0adXCC8t2019k8vf5QQEDQKlVFCx24QHrxjHxn3lPPlFjr/LCQgaBEqpoHPemAS+P6of89fk0ax3GmsQKKWC00XjB7C/vI51ebpojaVBICIzRSRbRHJEZN5R9rlGRLJEZLOIvGZlPUop1eKcUf1x2oWPN+z3dyl+Z9ni9SJiB54GzgXygFUissAYk9Vmn2HAXcB0Y0ypiPSzqh6llGorOszJtMF9+HZ7kb9L8TsrWwRTgBxjzE5jTAPwBjCr3T63AU8bY0oBjDEFFtajlFKHmTa4D9kHKympbvB3KX5lZRAkAW0n9MjzbGtrODBcRJaKyHIRmentQCIyV0QyRSSzsFBvDVdK+ca0wb0BWLmr2M+V+JdlXUOdeP9hwFlAMrBYRMYZYw4bvTHGPAs8C5CRkaFD/EopnxiXFEuY08bynSXYbTa2HazkZ2cP9XdZXc7KINgHpLR5nOzZ1lYesMIY0wjsEpFtuINhlYV1KaUU4F7GcsbQvryduZe3M/dS0+hiVnoiyXER/i6tS1nZNbQKGCYig0QkBJgNLGi3z/u4WwOISF/cXUU7LaxJKaUO88Dl44gKc7aucTx/dfvvqz2fZUFgjGkCbgcWAluAt4wxm0XkXhG51LPbQqBYRLKAr4A7jTHB3VmnlOpS/aPDWHD7dD64YwanDenD26v3Bt1yltLdpmLNyMgwmZmZ/i5DKdUDfbxxPz99dQ1/v3ESM8cm+LscnxKR1caYDG/P6Z3FSinlcf6YBFJ6h/Pct8HVQ61BoJRSHnabcPOpaazeU8qW/RUs31kcFAvYaBAopVQbl6YnYhP4wQsrmf3schasz/d3SZbTIFBKqTb6RYUxdVAfCivrAfjfz7Kpa3Tx6oo9lPbQO5A1CJRSqp1rJ6cQEWLnT5eOYW9JLfPmb+AP723ijws2+7s0S2gQKKVUO7PSE1nz3+dy07RUBvaO4P117u6hBevz+W5Hz7vCXYNAKaXaERHCnHZsNuGajGQArpuSQnJcOH9csIlGV7OfK/QtDQKllDqGaycP5PRhfZl7xhDuvng02w5WcfMLK9l6oMLfpfmMBoFSSh1DfFQor9w6lUF9e3Hu6P7Mu2AkW/ZXcMdra2nqIS0DDQKllOogEeEnZw7hoSvHs72gil+8uY6NeeX+LuukaRAopVQnnTe6P9dNSeHLLQXc/OLKbr+wjQaBUkp1kojw4BXjee9np1FR28gjC7f6u6STokGglFInaGRCNFdnJPP+2nxqGpr8Xc4J0yBQSqmTMCs9idpGF4u2dN8l1zUIlFLqJExO603/6FAWrOu+C9poECil1Emw24SrJiXzxdYCsvK7570FGgRKKXWS5p4+hOgwJ7+fv4FVu0v8XU6naRAopdRJiolwcu+sMeSW1HD9c8u73eWkGgRKKeUDs9KTeP22aTS6DB9v3O/vcjpFg0AppXxk1IAohvaL7HaL2WgQKKWUj4gIsyYksmp3CdsOVvq7nA7TIFBKKR+6cVoqvUIcPPrZNn+X0mEaBEop5UNxvUK4dcYgPt18gB2FVa3b65tcfqzq2DQIlFLKx66fOhARWOBZ2aykuoGJ937OvwP0pjMNAqWU8rH+0WGcOrgPC9bnY4xhc345NQ0unl+yy9+leaVBoJRSFpiVnsiuomrW5Jay7aC7i2hDXnlArl+gQaCUUha4eHwi0WEOnl+yi+0HK4kKcxDutPPayj3+Lu0IGgRKKWWBXqEOrp+ayqebDrBoSwGjB0RzyYQB/HtdPhV1jf4u7zAaBEopZZEfTk/DYbdRVFXP8P5R3DA1lZoGF/9eG1iDxhoESillkf7RYVw0bgAACTFhjE+OYWxSNK+uyMUY4+fqDtEgUEopC/33xaO5aNwAZqUnIiLcMDWVrQcqWZNb5u/SWlkaBCIyU0SyRSRHROZ5eX6OiBSKyDrPvx9ZWY9SSnW13r1CePqGU0iOiwDg0gmJRIY6eHVF4AwaWxYEImIHngYuAEYD14nIaC+7vmmMSff8+6dV9SilVCDoFergsomJfLhhP2U1gTFdtZUtgilAjjFmpzGmAXgDmGXh+ymlVLdw/ZRUGpqaeWd1nr9LAawNgiRgb5vHeZ5t7V0pIhtE5B0RSfF2IBGZKyKZIpJZWFhoRa1KKdVlRidGM3VQb57+Koeiqnp/l+P3weIPgDRjzHjgc+D/vO1kjHnWGJNhjMmIj4/v0gKVUsoK9182lup6F796cx01DU1+rcXKINgHtP2Gn+zZ1soYU2yMaYnDfwKTLKxHKaUCxrD+Udx32RiW5hTx41dW+/VyUiuDYBUwTEQGiUgIMBtY0HYHERnQ5uGlwBYL61FKqYBy7eSB/OeFo/h2exHf7Sj2Wx2WBYExpgm4HViI+wP+LWPMZhG5V0Qu9ez2cxHZLCLrgZ8Dc6yqRymlAtGN01JJiA7jsUXb/NYqkEC6u60jMjIyTGZmpr/LUEopn3n5u93c/e/N/OvWqcwY1teS9xCR1caYDG/P+XuwWCmlgt61k1MYEBPGo59nt7YKKrtwYjoNAqWU8rNQh52fnzOMNbllfLzxAO+uyWPSfYvIKag6/ot9QINAKaUCwDUZKYxMiOKBj7J48sscGlzNvLi0a1Y00yBQSqkAYLcJ/3PleMpqG9lVVE1yXDjvrtlHabX101BoECilVICYkBLLK7dOYe4Zg3nuBxk0uJp55LNsy9/XYfk7KKWU6rBJqb2ZlNobgJtPTePFZbu4fspAxibFWPae2iJQSqkA9ctzhxEV6uCvi7Zb+j4aBEopFaCiw5z86PTBLNpykE37yi17Hw0CpZQKYHOmpxEd5uDxL6xrFWgQKKVUAIsOc3LrjMF8nmVdq0CDQCmlAtyc6WkMiAlje0GlJcfXq4aUUirAxYQ7Wfy7s3Harfnuri0CpZTqBqwKAdAgUEqpoKdBoJRSQU6DQCmlgpwGgVJKBTkNAqWUCnIaBEopFeQ0CJRSKsh1u8XrRaQQ2HMSh4gBOnufdkdec6x9jvZc++3e9mu7rf3zfYGi49R1Ijp7jjq6f2fPUUe2HetxoJyfjr7GF39D3rbpOdJzBJBqjIn3+gpjTFD9A5614jXH2udoz7Xf7m2/ttu87J8ZCOeoo/t39hx1ZNuxHgfK+enKvyE9R3qOTuS4wdg19IFFrznWPkd7rv12b/t9cJznrdDZ9+no/p09Rx3Z1pFz6GuB/DfkbZueo+NvC5Zz5FW36xpSh4hIpjEmw991BCo9P8en5+j4guEcBWOLoCd51t8FBDg9P8en5+j4evw50haBUkoFOW0RKKVUkNMgUEqpIKdBoJRSQU6DoAcSEZuIPCAiT4rIzf6uJxCJyFki8q2I/F1EzvJ3PYFKRHqJSKaIXOzvWgKRiIzy/A29IyL/4e96TpQGQYARkRdEpEBENrXbPlNEskUkR0TmHecws4BkoBHIs6pWf/HROTJAFRCGnqNj+T3wljVV+pcvzpExZosx5ifANcB0K+u1kl41FGBE5AzcH1AvG2PGerbZgW3Aubg/tFYB1wF24MF2h7jF86/UGPMPEXnHGHNVV9XfFXx0joqMMc0i0h941BhzQ1fV3xV8dI4mAH1wh2WRMebDrqm+a/jiHBljCkTkUuA/gFeMMa91Vf2+pIvXBxhjzGIRSWu3eQqQY4zZCSAibwCzjDEPAkc02UUkD2jwPHRZV61/+OIctVEKhFpRpz/56O/oLKAXMBqoFZGPjTHNVtbdlXz1d2SMWQAsEJGPAA0CZZkkYG+bx3nA1GPs/y7wpIicDiy2srAA0qlzJCJXAOcDscBT1pYWMDp1jowxfwAQkTl4WlCWVhcYOvt3dBZwBe4vEx9bWpmFNAh6IGNMDXCrv+sIZMaYd3EHpjoOY8xL/q4hUBljvga+9nMZJ00Hi7uHfUBKm8fJnm3qED1Hx6fn6PiC8hxpEHQPq4BhIjJIREKA2cACP9cUaPQcHZ+eo+MLynOkQRBgROR14DtghIjkicitxpgm4HZgIbAFeMsYs9mfdfqTnqPj03N0fHqODtHLR5VSKshpi0AppYKcBoFSSgU5DQKllApyGgRKKRXkNAiUUirIaRAopVSQ0yBQlhORqi54j5+IyA+sfp9273mZiIw+wdfd7fn5HhH5re+r6zzPGg3HnGFURMaJyEtdVJLqIjrXkOo2RMRujPE6m6ox5u9d/Z7AZcCHQFYnD/s74NKTKsxPjDEbRSRZRAYaY3L9XY/yDW0RqC4lIneKyCoR2SAif2qz/X0RWS0im0VkbpvtVSLyFxFZD5zqefyAiKwXkeWe9QQO+2YtIl+LyP+IyEoR2eaZhRURiRCRt0QkS0TeE5EVIpLhpcbdntevAa4Wkds8Na8Xkfme45yG+8P8ERFZJyJDPP8+9fwe34rISC/HHg7UG2OKvDyX7vmdNnjqi/Nsn+zZtk5EHmm/kIpnnwEistizz6Y2v/NMEVnjqf0Lz7YpIvKdiKwVkWUiMsLL8XqJe+GWlZ79ZrV5+gPcUy+oHkKDQHUZETkPGIZ7zvd0YJK4FwcB9yIfk4AM4Oci0sezvRewwhgzwRizxPN4uTFmAu4ptm87yts5jDFTgF8Cf/Rs+ynuBXtGA/8NTDpGucXGmFOMMW8A7xpjJnvecwtwqzFmGe45aO40xqQbY3YAzwJ3eH6P3wJ/83Lc6cCao7zny8DvjTHjgY1t6n4R+LExJp2jry9xPbDQs88EYJ2IxAPPAVd6ar/as+9W4HRjzETgbuDPXo73B+BLzzk8G3fg9fI8lwmcfpQ6VDekXUOqK53n+bfW8zgSdzAsxv3hf7lne4pnezHuD775bY7RgLs7BmA17pWkvHm3zT5pnp9nAI8DGGM2iciGY9T6Zpufx4rI/bjXLojEPQ/NYUQkEjgNeFtEWjZ7W/BmAFDo5fUxQKwx5hvPpv/zHCsWiDLGfOfZ/hreF0hZBbwgIk7gfWPMOnHPlb/YGLPL8zuXePaNAf5PRIbhXrLT6eV45wGXthm/CAMG4g7CAiDRy2tUN6VBoLqSAA8aY/5x2Eb3B9b3gVONMTUi8jXuDx6AunZ99I3m0ARZLo7+N1zfgX2OpbrNzy8Blxlj1ot7kZazvOxvA8o838iPpRb3B7FPeVbbOgO4CHhJRB7FvfqaN/cBXxljLhf3Cl1fe9lHcLcksr08F4b791A9hHYNqa60ELjF8+0ZEUkSkX64PxhLPSEwEphm0fsvxb3IOJ6rfcZ18HVRwH7Pt+22axtXep7DGFMB7BKRqz3HFxGZ4OVYW4Ch7TcaY8qB0pa+feAm4BtjTBlQKSItq2R57ZsXkVTgoDHmOeCfwCnAcuAMERnk2ae3Z/cYDs2xP+cov/NC4A7xNG9EZGKb54YDR4xTqO5Lg0B1GWPMZ7i7Nr4TkY3AO7g/SD8FHCKyBXgI9weYFf4GxItIFnA/sBko78Dr/htYgTtItrbZ/gZwp2cwdQjukLjVM7C9GZh1xJHc3WATWz5g27kZd1/8BtxjKPd6tt8KPCci63CPkXir+SxgvYisBa4FHjfGFAJzgXc9NbV0dz0MPOjZ92itpftwdxltEJHNnsctzgY+OsrrVDek01CroCEidsBpjKnzfHAvAkYYYxq6uI7HgQ+MMYs6uH+kMabK8/M8YIAx5hdW1niMWkKBb4AZnrn7VQ+gYwQqmEQAX3m6eAT4aVeHgMefOcaC6F5cJCJ34f7/uoejd+d0hYHAPA2BnkVbBEopFeR0jEAppYKcBoFSSgU5DQKllApyGgRKKRXkNAiUUirIaRAopVSQ+3+nVQcrk6AKUAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfcHgZYY3ELi",
        "outputId": "67855b9a-94f4-446a-c557-a033ba3f0bc3"
      },
      "source": [
        "for i, j in zip(lr_finder.losses, lr_finder.lrs):\n",
        "  if i == lr_finder.best_loss:\n",
        "    print(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0029917376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zD6jdw4hZwdc",
        "outputId": "6be9d164-1ad0-4557-b601-cbb4d751e297"
      },
      "source": [
        "#bs = 32\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs= epochs, \n",
        "                    batch_size= bs, \n",
        "                    validation_split= 0.05, \n",
        "                    verbose= 1,\n",
        "                    shuffle=False,\n",
        "                    callbacks= [reduce_lr, es])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "117/117 [==============================] - 79s 640ms/step - loss: 1130709385216.0000 - val_loss: 9295299584.0000\n",
            "Epoch 2/50\n",
            "117/117 [==============================] - 75s 637ms/step - loss: 2716397469696.0000 - val_loss: 4269.0068\n",
            "Epoch 3/50\n",
            "117/117 [==============================] - 72s 618ms/step - loss: 7.1968 - val_loss: 1660.4292\n",
            "Epoch 4/50\n",
            "117/117 [==============================] - 71s 609ms/step - loss: 1.2890 - val_loss: 951.8007\n",
            "Epoch 5/50\n",
            "117/117 [==============================] - 71s 606ms/step - loss: 1.2849 - val_loss: 1196.6124\n",
            "Epoch 6/50\n",
            "117/117 [==============================] - 71s 608ms/step - loss: 1.0773 - val_loss: 2074.0393\n",
            "Epoch 7/50\n",
            "117/117 [==============================] - 71s 606ms/step - loss: 0.9748 - val_loss: 9591.2803\n",
            "Epoch 8/50\n",
            "117/117 [==============================] - 72s 611ms/step - loss: 1.0440 - val_loss: 5947.2417\n",
            "Epoch 9/50\n",
            "117/117 [==============================] - 73s 621ms/step - loss: 0.9091 - val_loss: 6375.9658\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "Epoch 10/50\n",
            "117/117 [==============================] - 75s 638ms/step - loss: 0.8796 - val_loss: 6687.8882\n",
            "Epoch 11/50\n",
            "117/117 [==============================] - 74s 634ms/step - loss: 0.9106 - val_loss: 2574.4185\n",
            "Epoch 12/50\n",
            "117/117 [==============================] - 71s 610ms/step - loss: 0.8802 - val_loss: 969.0195\n",
            "Epoch 13/50\n",
            "117/117 [==============================] - 71s 611ms/step - loss: 0.8194 - val_loss: 242.8442\n",
            "Epoch 14/50\n",
            "117/117 [==============================] - 70s 602ms/step - loss: 0.8886 - val_loss: 282.8273\n",
            "Epoch 15/50\n",
            "117/117 [==============================] - 72s 616ms/step - loss: 0.8738 - val_loss: 80.3759\n",
            "Epoch 16/50\n",
            "117/117 [==============================] - 73s 621ms/step - loss: 0.7349 - val_loss: 276.2424\n",
            "Epoch 17/50\n",
            "117/117 [==============================] - 74s 630ms/step - loss: 0.7425 - val_loss: 262.4519\n",
            "Epoch 18/50\n",
            "117/117 [==============================] - 74s 633ms/step - loss: 0.7395 - val_loss: 272.7699\n",
            "Epoch 19/50\n",
            "117/117 [==============================] - 72s 616ms/step - loss: 0.7742 - val_loss: 164.1078\n",
            "Epoch 20/50\n",
            "117/117 [==============================] - 74s 637ms/step - loss: 1.5813 - val_loss: 58.0719\n",
            "Epoch 21/50\n",
            "117/117 [==============================] - 75s 640ms/step - loss: 0.9842 - val_loss: 41.9671\n",
            "Epoch 22/50\n",
            "117/117 [==============================] - 75s 643ms/step - loss: 0.9182 - val_loss: 34.3960\n",
            "Epoch 23/50\n",
            "117/117 [==============================] - 73s 621ms/step - loss: 0.8637 - val_loss: 54.8660\n",
            "Epoch 24/50\n",
            "117/117 [==============================] - 75s 641ms/step - loss: 0.8379 - val_loss: 103.1270\n",
            "Epoch 25/50\n",
            "117/117 [==============================] - 74s 637ms/step - loss: 0.7982 - val_loss: 59.7764\n",
            "Epoch 26/50\n",
            "117/117 [==============================] - 74s 635ms/step - loss: nan - val_loss: nan\n",
            "Epoch 27/50\n",
            " 85/117 [====================>.........] - ETA: 20s - loss: nan"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5ddfb15ee6b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     callbacks= [reduce_lr, es])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czMZAGOcVE8C"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG5J8sF9XaND"
      },
      "source": [
        "# from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "# from pandas.tseries.offsets import CustomBusinessDay\n",
        "# us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
        "\n",
        "# n_past = 16\n",
        "# n_days_for_prediction=15  #let us predict past 15 days\n",
        "\n",
        "# predict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_days_for_prediction, freq=us_bd).tolist()\n",
        "# print(predict_period_dates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNetBqwMSPcn"
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yBERsGsS3ty"
      },
      "source": [
        "y_pred = Y_scaler.inverse_transform(y_pred)\n",
        "y_test = Y_scaler.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFrorRvATAfG"
      },
      "source": [
        "plt.figure(figsize= (14,5))\n",
        "plt.plot(y_test, color= 'red', label= 'Real Google Stock Price')\n",
        "plt.plot(y_pred, color= 'blue', label= 'Predicted Google Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}